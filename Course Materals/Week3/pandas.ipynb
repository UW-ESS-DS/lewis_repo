{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-scoop",
   "metadata": {},
   "source": [
    "# Introduction to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-doctor",
   "metadata": {},
   "source": [
    "The python package pandas is very useful to read csv files, but also many text files that are more or less formated as one observation per row and one column for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-developer",
   "metadata": {},
   "source": [
    "As an example, we are going to look at the list of seismic stations from the Northern California seismic network, available here:\n",
    "\n",
    "http://ncedc.org/ftp/pub/doc/NC.info/NC.channel.summary.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://ncedc.org/ftp/pub/doc/NC.info/NC.channel.summary.day'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-cartoon",
   "metadata": {},
   "source": [
    "First we import useful packages. The package request is useful to read data from a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bacterial-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import pickle\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from math import cos, sin, pi, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-exclusive",
   "metadata": {},
   "source": [
    "The function read_csv is used to open and read your text file. In the case of a well formatted csv file, only the name of the file needs to be entered:\n",
    "\n",
    "data = pd.read_csv('my_file.csv')\n",
    "\n",
    "However, many options are available if the file is not well formatted. See more on:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "graduate-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.get(url).content\n",
    "data = pd.read_csv(io.StringIO(s.decode('utf-8')), header=None, skiprows=2, sep='\\s+', usecols=list(range(0, 13)))\n",
    "data.columns = ['station', 'network', 'channel', 'location', 'rate', 'start_time', 'end_time', 'latitude', 'longitude', 'elevation', 'depth', 'dip', 'azimuth']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-plate",
   "metadata": {},
   "source": [
    "Let us look at the data. They are now stored into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-automation",
   "metadata": {},
   "source": [
    "There are two aways of looking at a particular column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['station']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-platinum",
   "metadata": {},
   "source": [
    "If we want to look at a given row or column, and we know its index, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-interest",
   "metadata": {},
   "source": [
    "If we know the name of the column, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, 'station']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-petersburg",
   "metadata": {},
   "source": [
    "We can also access a single value within a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0, 'station']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-desire",
   "metadata": {},
   "source": [
    "We can filter the data with the value taken by a given column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complicated-flesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>network</th>\n",
       "      <th>channel</th>\n",
       "      <th>location</th>\n",
       "      <th>rate</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>depth</th>\n",
       "      <th>dip</th>\n",
       "      <th>azimuth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1999/08/03,00:00:00</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>2002/01/24,23:50:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2002/01/24,23:50:00</td>\n",
       "      <td>2002/10/16,23:59:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2002/10/17,00:00:00</td>\n",
       "      <td>2006/01/24,18:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHN</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1999/08/03,00:00:00</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3066</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>MNE</td>\n",
       "      <td>--</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>2000/07/12,00:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>MNN</td>\n",
       "      <td>--</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1999/08/03,00:00:00</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>MNN</td>\n",
       "      <td>--</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>2000/07/12,00:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>MNZ</td>\n",
       "      <td>--</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1999/08/03,00:00:00</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>MNZ</td>\n",
       "      <td>--</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>2000/07/12,00:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     station network channel location  rate           start_time  \\\n",
       "2993    KCPB      NC     BHE       --  50.0  1999/08/03,00:00:00   \n",
       "2994    KCPB      NC     BHE       --  50.0  2000/06/06,16:00:00   \n",
       "2995    KCPB      NC     BHE       --  50.0  2002/01/24,23:50:00   \n",
       "2996    KCPB      NC     BHE       --  20.0  2002/10/17,00:00:00   \n",
       "2997    KCPB      NC     BHN       --  50.0  1999/08/03,00:00:00   \n",
       "...      ...     ...     ...      ...   ...                  ...   \n",
       "3066    KCPB      NC     MNE       --  10.0  2000/06/06,16:00:00   \n",
       "3067    KCPB      NC     MNN       --  10.0  1999/08/03,00:00:00   \n",
       "3068    KCPB      NC     MNN       --  10.0  2000/06/06,16:00:00   \n",
       "3069    KCPB      NC     MNZ       --  10.0  1999/08/03,00:00:00   \n",
       "3070    KCPB      NC     MNZ       --  10.0  2000/06/06,16:00:00   \n",
       "\n",
       "                 end_time  latitude  longitude  elevation  depth   dip  \\\n",
       "2993  2000/06/06,16:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2994  2002/01/24,23:50:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2995  2002/10/16,23:59:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2996  2006/01/24,18:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2997  2000/06/06,16:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "...                   ...       ...        ...        ...    ...   ...   \n",
       "3066  2000/07/12,00:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "3067  2000/06/06,16:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "3068  2000/07/12,00:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "3069  2000/06/06,16:00:00  39.68631 -123.58242     1261.0    0.0 -90.0   \n",
       "3070  2000/07/12,00:00:00  39.68631 -123.58242     1261.0    0.0 -90.0   \n",
       "\n",
       "      azimuth  \n",
       "2993     90.0  \n",
       "2994     90.0  \n",
       "2995     90.0  \n",
       "2996     90.0  \n",
       "2997      0.0  \n",
       "...       ...  \n",
       "3066     90.0  \n",
       "3067      0.0  \n",
       "3068      0.0  \n",
       "3069      0.0  \n",
       "3070      0.0  \n",
       "\n",
       "[78 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.station=='KCPB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "buried-causing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>network</th>\n",
       "      <th>channel</th>\n",
       "      <th>location</th>\n",
       "      <th>rate</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>depth</th>\n",
       "      <th>dip</th>\n",
       "      <th>azimuth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1999/08/03,00:00:00</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>2002/01/24,23:50:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2002/01/24,23:50:00</td>\n",
       "      <td>2002/10/16,23:59:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHE</td>\n",
       "      <td>--</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2002/10/17,00:00:00</td>\n",
       "      <td>2006/01/24,18:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>KCPB</td>\n",
       "      <td>NC</td>\n",
       "      <td>BHN</td>\n",
       "      <td>--</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1999/08/03,00:00:00</td>\n",
       "      <td>2000/06/06,16:00:00</td>\n",
       "      <td>39.68631</td>\n",
       "      <td>-123.58242</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>KHBB</td>\n",
       "      <td>NC</td>\n",
       "      <td>LHZ</td>\n",
       "      <td>--</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015/10/29,21:18:00</td>\n",
       "      <td>2016/04/28,16:56:00</td>\n",
       "      <td>40.65990</td>\n",
       "      <td>-123.21966</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>KHBB</td>\n",
       "      <td>NC</td>\n",
       "      <td>LHZ</td>\n",
       "      <td>--</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016/04/28,16:56:00</td>\n",
       "      <td>3000/01/01,00:00:00</td>\n",
       "      <td>40.65990</td>\n",
       "      <td>-123.21966</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>KHBB</td>\n",
       "      <td>NC</td>\n",
       "      <td>LNE</td>\n",
       "      <td>--</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015/10/29,21:18:00</td>\n",
       "      <td>2016/04/28,16:56:00</td>\n",
       "      <td>40.65990</td>\n",
       "      <td>-123.21966</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>KHBB</td>\n",
       "      <td>NC</td>\n",
       "      <td>LNN</td>\n",
       "      <td>--</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015/10/29,21:18:00</td>\n",
       "      <td>2016/04/28,16:56:00</td>\n",
       "      <td>40.65990</td>\n",
       "      <td>-123.21966</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>KHBB</td>\n",
       "      <td>NC</td>\n",
       "      <td>LNZ</td>\n",
       "      <td>--</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015/10/29,21:18:00</td>\n",
       "      <td>2016/04/28,16:56:00</td>\n",
       "      <td>40.65990</td>\n",
       "      <td>-123.21966</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     station network channel location  rate           start_time  \\\n",
       "2993    KCPB      NC     BHE       --  50.0  1999/08/03,00:00:00   \n",
       "2994    KCPB      NC     BHE       --  50.0  2000/06/06,16:00:00   \n",
       "2995    KCPB      NC     BHE       --  50.0  2002/01/24,23:50:00   \n",
       "2996    KCPB      NC     BHE       --  20.0  2002/10/17,00:00:00   \n",
       "2997    KCPB      NC     BHN       --  50.0  1999/08/03,00:00:00   \n",
       "...      ...     ...     ...      ...   ...                  ...   \n",
       "3212    KHBB      NC     LHZ       --   1.0  2015/10/29,21:18:00   \n",
       "3213    KHBB      NC     LHZ       --   1.0  2016/04/28,16:56:00   \n",
       "3214    KHBB      NC     LNE       --   1.0  2015/10/29,21:18:00   \n",
       "3215    KHBB      NC     LNN       --   1.0  2015/10/29,21:18:00   \n",
       "3216    KHBB      NC     LNZ       --   1.0  2015/10/29,21:18:00   \n",
       "\n",
       "                 end_time  latitude  longitude  elevation  depth   dip  \\\n",
       "2993  2000/06/06,16:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2994  2002/01/24,23:50:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2995  2002/10/16,23:59:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2996  2006/01/24,18:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "2997  2000/06/06,16:00:00  39.68631 -123.58242     1261.0    0.0   0.0   \n",
       "...                   ...       ...        ...        ...    ...   ...   \n",
       "3212  2016/04/28,16:56:00  40.65990 -123.21966     1864.0    0.0 -90.0   \n",
       "3213  3000/01/01,00:00:00  40.65990 -123.21966     1864.0    0.0 -90.0   \n",
       "3214  2016/04/28,16:56:00  40.65990 -123.21966     1864.0    0.0   0.0   \n",
       "3215  2016/04/28,16:56:00  40.65990 -123.21966     1864.0    0.0   0.0   \n",
       "3216  2016/04/28,16:56:00  40.65990 -123.21966     1864.0    0.0 -90.0   \n",
       "\n",
       "      azimuth  \n",
       "2993     90.0  \n",
       "2994     90.0  \n",
       "2995     90.0  \n",
       "2996     90.0  \n",
       "2997      0.0  \n",
       "...       ...  \n",
       "3212      0.0  \n",
       "3213      0.0  \n",
       "3214     90.0  \n",
       "3215      0.0  \n",
       "3216      0.0  \n",
       "\n",
       "[117 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[(data.station=='KCPB') | (data.station=='KHBB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.station.isin(['KCPB', 'KHBB'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-coral",
   "metadata": {},
   "source": [
    "We can access to a brief summary of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.station.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.elevation.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-hotel",
   "metadata": {},
   "source": [
    "We can perform standard operations on the whole data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "further-transmission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rate          92.544982\n",
       "latitude      37.930985\n",
       "longitude   -121.440088\n",
       "elevation    650.942248\n",
       "depth         17.581270\n",
       "dip          -45.894137\n",
       "azimuth       18.549186\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-bhutan",
   "metadata": {},
   "source": [
    "In the case of a categorical variable, we can get the list of possile values that this variable can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "voluntary-asian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EHZ', 'SHZ', 'HHE', 'HHN', 'HHZ', 'HNE', 'HNN', 'HNZ', 'LHE',\n",
       "       'LHN', 'LHZ', 'ELE', 'ELN', 'ELZ', 'SLE', 'SLN', 'SLZ', 'LCE',\n",
       "       'LCL', 'LCQ', 'LOG', 'OCF', 'VCO', 'VEA', 'VEC', 'VEP', 'VFP',\n",
       "       'VKI', 'GAN', 'GNS', 'GPL', 'GST', 'VDT', 'VEI', 'VPB', 'EHE',\n",
       "       'EHN', 'BNE', 'BNN', 'BN1', 'BN2', 'BN3', 'BV1', 'EP1', 'EP2',\n",
       "       'EP3', 'HDO', 'HN1', 'HN2', 'HN3', 'HV1', 'SP2', 'SP3', 'BNZ',\n",
       "       'LDO', 'HJ2', 'HJ3', 'HJZ', 'ACE', 'GEL', 'GLA', 'GLO', 'ATT',\n",
       "       'SHE', 'SHN', 'BHE', 'BHN', 'BHZ', 'LNE', 'LNN', 'LNZ', 'MHE',\n",
       "       'MHN', 'MHZ', 'MNE', 'MNN', 'MNZ', 'HH2', 'HH3', 'LH2', 'LH3',\n",
       "       'SP1', 'DP1', 'DP2', 'DP3', 'HLE', 'HLN', 'HLZ', 'XNE', 'XNN',\n",
       "       'XNZ', 'EH1'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.channel.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-principal",
   "metadata": {},
   "source": [
    "and get the number of times that each value is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.station.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-offense",
   "metadata": {},
   "source": [
    "There are several ways of doing an operation on all rows of a column. The first option is to use the map function.\n",
    "\n",
    "If you are not familiar with lambda function in Python, look at:\n",
    "\n",
    "https://realpython.com/python-lambda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_elevation_mean = data.elevation.mean()\n",
    "data.elevation.map(lambda p: p - data_elevation_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-professional",
   "metadata": {},
   "source": [
    "The second option is to use the apply function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remean_elevation(row):\n",
    "    row.elevation = row.elevation - data_elevation_mean\n",
    "    return row\n",
    "data.apply(remean_elevation, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-trick",
   "metadata": {},
   "source": [
    "We can also carry out simple operations on coulumns, provided they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.network + ' - ' + data.station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-upgrade",
   "metadata": {},
   "source": [
    "A useful feature is to group the rows depending on the value of a categorical variable, and then apply the same operation to all the groups. For instance, I want to know how many times each station appears in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('station').station.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-least",
   "metadata": {},
   "source": [
    "Or I want to know what is the lowest and the highest elevation for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('station').elevation.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('station').elevation.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-sixth",
   "metadata": {},
   "source": [
    "We can have access to the data type of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-special",
   "metadata": {},
   "source": [
    "Here, pandas does not recognize the start_time and end_time columns as a datetime format, so we cannot use datetime operations on them. We first need to convert these columns into a datetime format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform column into datetime format\n",
    "startdate = pd.to_datetime(data['start_time'], format='%Y/%m/%d,%H:%M:%S')\n",
    "data['start_time'] = startdate\n",
    "# Avoid 'OutOfBoundsDatetime' error with year 3000\n",
    "enddate = data['end_time'].str.replace('3000', '2025')\n",
    "enddate = pd.to_datetime(enddate, format='%Y/%m/%d,%H:%M:%S')\n",
    "data['end_time'] = enddate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-surprise",
   "metadata": {},
   "source": [
    "We can now look when each seismic station was installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('station').apply(lambda df: df.start_time.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-jungle",
   "metadata": {},
   "source": [
    "The agg function allows to carry out several operations to each group of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['station']).elevation.agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['station']).agg({'start_time':lambda x: min(x), 'end_time':lambda x: max(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-concern",
   "metadata": {},
   "source": [
    "We can also make groups by selecting the values of two categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['station', 'channel']).agg({'start_time':lambda x: min(x), 'end_time':lambda x: max(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-cloud",
   "metadata": {},
   "source": [
    "Previously, we just printed the output, but we can also store it in a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped = data.groupby(['station', 'channel']).agg({'start_time':lambda x: min(x), 'end_time':lambda x: max(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-breach",
   "metadata": {},
   "source": [
    "When we select only some rows, the index is not automatically reset to start at 0. We can do it manually. Many functions in pandas have also an option to reset the index, and option to transform the dataframe in place, instead of saving the results in another variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-combination",
   "metadata": {},
   "source": [
    "It is also possible to sort the dataset by value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped.sort_values(by='start_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-matter",
   "metadata": {},
   "source": [
    "We can apply the sorting to several columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped.sort_values(by=['start_time', 'end_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-irish",
   "metadata": {},
   "source": [
    "A useful pandas function is the merge functions that allows you two merge two dataframes that have some columns in common, but have also different columns that you may want to compare with each other.\n",
    "\n",
    "For example, I have two earthquake catalogs. The 2007-2009 was established using data from a temporary experiment, and the 2004-2011 was established using data from a permanent seismic network. I would like to know if some earthquakes are detected by a network, but not by the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-hospital",
   "metadata": {},
   "source": [
    "I will compare the catalogs between July 2007 and May 2009. There is a time delay of 10s between the detection time of one catalog compared to the other. I will also filter the catalogs to eleiminate false detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbegin = datetime(2007, 9, 25, 0, 0, 0)\n",
    "tend = datetime(2009, 5, 14, 0, 0, 0)\n",
    "dt = 10.0\n",
    "thresh1 = 1.4\n",
    "thresh2 = 1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-basement",
   "metadata": {},
   "source": [
    "I first read the two catalogs, and apply the filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "namefile = 'catalog_2007_2009.pkl'\n",
    "df1 = pickle.load(open(namefile, 'rb'))\n",
    "df1 = df1[['year', 'month', 'day', 'hour', 'minute', 'second', 'cc', 'nchannel']]\n",
    "df1 = df1.astype({'year': int, 'month': int, 'day': int, 'hour': int, 'minute': int, 'second': float, 'cc': float, 'nchannel': int})\n",
    "date = pd.to_datetime(df1.drop(columns=['cc', 'nchannel']))\n",
    "df1['date'] = date\n",
    "df1 = df1[(df1['date'] >= tbegin) & (df1['date'] <= tend)]\n",
    "df1_filter = df1.loc[df1['cc'] * df1['nchannel'] >= thresh1]\n",
    "\n",
    "namefile = 'catalog_2004_2011.pkl'\n",
    "df2 = pickle.load(open(namefile, 'rb'))\n",
    "df2 = df2[['year', 'month', 'day', 'hour', 'minute', 'second', 'cc', 'nchannel']]\n",
    "df2 = df2.astype({'year': int, 'month': int, 'day': int, 'hour': int, 'minute': int, 'second': float, 'cc': float, 'nchannel': int})\n",
    "date = pd.to_datetime(df2.drop(columns=['cc', 'nchannel']))\n",
    "df2['date'] = date\n",
    "df2['date'] = df2['date'] - timedelta(seconds=dt)\n",
    "df2 = df2[(df2['date'] >= tbegin) & (df2['date'] <= tend)]\n",
    "df2_filter = df2.loc[df2['cc'] * df2['nchannel'] >= thresh2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-hypothesis",
   "metadata": {},
   "source": [
    "To make the comparison, I first concatenate the two dataframes into a single dataframe. Then I merge the concatenated dataframe with one of the initial dataframes.\n",
    "\n",
    "I apply the merge operation on the date column, that is if an earthquake in dataset 1 has the same date as an earthquake in dataset 2, I assume it is the same earthquake. You could also check if several columns have the same value, instead of doing the merge operation on only one column.\n",
    "\n",
    "The process adds a merge column to the dataset, which indicates whether a row was found only in dataset 1, only in dataset 2, or in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earthquakes in filtered 2007-2009 catalog but not in (unfiltered) 2004-2011 catalog\n",
    "df_all = pd.concat([df2, df1_filter], ignore_index=True)\n",
    "df_merge = df_all.merge(df2.drop_duplicates(), on=['date'], how='left', indicator=True)\n",
    "df_added_1 = df_merge[df_merge['_merge'] == 'left_only']\n",
    "\n",
    "# Earthquakes in filtered 2004-2011 catalog but not in (unfiltered) 2007-2009 catalog\n",
    "df_all = pd.concat([df1, df2_filter], ignore_index=True)\n",
    "df_merge = df_all.merge(df1.drop_duplicates(), on=['date'], how='left', indicator=True)\n",
    "df_added_2 = df_merge[df_merge['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decreased-deposit",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_added_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1ab981dc199f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_added_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_added_1' is not defined"
     ]
    }
   ],
   "source": [
    "df_added_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_added_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-cache",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
